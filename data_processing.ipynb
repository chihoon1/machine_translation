{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4196d7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 17:20:14.488106: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "from data_processing import text_standardization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff34ee9",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18309f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: translate language in input_text to language in output_text\n",
    "input_text = pickle.load(open('data/Train_input', 'rb'))\n",
    "output_text = pickle.load(open('data/Train_output', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06f02ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of input and output texts with different lengths: 112000\n"
     ]
    }
   ],
   "source": [
    "# adding starting and end tokens\n",
    "count = 0\n",
    "text_pairs = []\n",
    "for i in range(len(output_text)):\n",
    "    out = \"[start] \" + output_text[i] + \"[end]\"\n",
    "    if len(output_text[i]) != len(input_text[i]):\n",
    "        count += 1\n",
    "    text_pairs.append((input_text[i], out))\n",
    "print(\"number of input and output texts with different lengths: %d\" % count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83746fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randomly selected input and output text pairs:\n",
      "('a f b d a g a g c g a f a f a h c d c f a f c f c g b d b d b d ', '[start] b d c g c d c f c f c g a f h i a h f g j b d a f k l b d a f m ed a g e ee b d a g ef eg a f d eh [end]')\n"
     ]
    }
   ],
   "source": [
    "# data shuffle\n",
    "print(f\"randomly selected input and output text pairs:\\n{random.choice(text_pairs)}\")\n",
    "random.shuffle(text_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d2a751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# train: 70%, val: 15%, and test: 15% of data\n",
    "trainval_pairs, test_pairs = train_test_split(text_pairs, test_size=0.15, random_state=3)\n",
    "train_pairs, val_pairs = train_test_split(trainval_pairs, test_size=0.15/.85, random_state=3)\n",
    "\n",
    "pickle.dump(train_pairs, open(\"data/train_pairs.pkl\", \"wb\"))\n",
    "pickle.dump(val_pairs, open(\"data/val_pairs.pkl\", \"wb\"))\n",
    "pickle.dump(test_pairs, open(\"data/test_pairs.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77ace8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions based on the dataset analysis\n",
    "input_vocab_size = 8 + 2  # +2 for \"\" and Unkown\n",
    "\n",
    "# max text length in both input and output text is 47\n",
    "# but I increased to 55 to account for longer texts that may exist in unseen data\n",
    "output_seq_len = 55\n",
    "input_seq_len = output_seq_len\n",
    "output_vocab_size = 18 + 2 + 2  # +2 for \"\" and Unkown and +2 for [start] and [end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4171ff81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 17:20:28.551098: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# initialize instances of vectorization layers both for source(input language) and target(output language)\n",
    "source_vectorization = layers.TextVectorization(\n",
    "    max_tokens = input_vocab_size,\n",
    "    output_mode = \"int\",\n",
    "    output_sequence_length = input_seq_len,\n",
    "    standardize = text_standardization,\n",
    ")\n",
    "target_vectorization = layers.TextVectorization(\n",
    "    max_tokens = output_vocab_size,\n",
    "    output_mode = \"int\",\n",
    "    output_sequence_length = output_seq_len + 1,\n",
    "    standardize = text_standardization,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c2180c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_texts = [pair[0] for pair in train_pairs]  # list of input texts in train data\n",
    "train_output_texts = [pair[1] for pair in train_pairs]  # list of output texts in train data\n",
    "# learn the vocabulary of each language to vectorize tokens with shape(seq_len,)\n",
    "# and each vocab will be assigned a number from 1:vocab_size and 0 for masking\n",
    "source_vectorization.adapt(train_input_texts)\n",
    "target_vectorization.adapt(train_output_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f373cfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the text vectorizations to load them when reloading the model\n",
    "pickle.dump({'config': target_vectorization.get_config(),\n",
    "             'weights': target_vectorization.get_weights()},\n",
    "           open(\"model/target_vectorization8.pkl\", \"wb\"))\n",
    "pickle.dump({'config': source_vectorization.get_config(),\n",
    "             'weights': source_vectorization.get_weights()},\n",
    "           open(\"model/source_vectorization8.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5cc05a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
