{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a907188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 22:30:44.725698: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from data_processing import text_standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ee00434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d6b1224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved splitted data\n",
    "train_pairs = pickle.load(open(\"data/train_pairs.pkl\", \"rb\"))\n",
    "val_pairs = pickle.load(open(\"data/val_pairs.pkl\", \"rb\"))\n",
    "test_pairs = pickle.load(open(\"data/test_pairs.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "506eaf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output vocabulary:\n",
      "['', '[UNK]', 'd', 'a', 'c', 'b', 'f', 'g', 'e', 'h', 'j', 'i', '[start]', '[end]', 'k', 'l', 'm', 'ed', 'ee', 'ef', 'eg', 'eh']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 22:30:48.589582: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# load vectorization for output language\n",
    "from_disk = pickle.load(open(\"model/target_vectorization8.pkl\", \"rb\"))\n",
    "target_vectorization = layers.TextVectorization.from_config(from_disk['config'])\n",
    "\n",
    "target_vectorization.set_weights(from_disk['weights'])\n",
    "\n",
    "\n",
    "print(f\"output vocabulary:\\n{target_vectorization.get_vocabulary()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a11c673b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input vocabulary:\n",
      "['', '[UNK]', 'a', 'd', 'c', 'b', 'f', 'g', 'e', 'h']\n"
     ]
    }
   ],
   "source": [
    "# load vectorization for input language\n",
    "from_disk = pickle.load(open(\"model/source_vectorization8.pkl\", \"rb\"))\n",
    "source_vectorization = layers.TextVectorization.from_config(from_disk['config'])\n",
    "\n",
    "source_vectorization.set_weights(from_disk['weights'])\n",
    "\n",
    "\n",
    "print(f\"input vocabulary:\\n{source_vectorization.get_vocabulary()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5601ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions based on the dataset analysis\n",
    "input_vocab_size = 8 + 2  # +2 for \"\" and Unkown\n",
    "\n",
    "# max text length in both input and output text is 47\n",
    "# but I increased to 55 to account for longer texts that may exist in unseen data\n",
    "output_seq_len = 55\n",
    "input_seq_len = output_seq_len\n",
    "output_vocab_size = 18 + 2 + 2  # +2 for \"\" and Unkown and +2 for [start] and [end]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41274cb9",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7eba09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cd9c66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(source, target):\n",
    "    source = source_vectorization(source)\n",
    "    target = target_vectorization(target)\n",
    "    return (\n",
    "    {\n",
    "        \"source\": source,\n",
    "        \"translated\": target[:, :-1]\n",
    "        \n",
    "    }, target[:, 1:])\n",
    "\n",
    "def make_dataset(source_target_pairs, **kwargs):\n",
    "    batch_size = kwargs.get(\"batch_size\", 64)\n",
    "    source_texts, target_texts = zip(*source_target_pairs)\n",
    "    source_texts = list(source_texts)\n",
    "    target_texts = list(target_texts)\n",
    "    # source and target data will be stored in tf dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((source_texts, target_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    # apply data preprocessing function as in the order they stored in tf dataset\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88b5c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = make_dataset(train_pairs, batch_size=batch_size)\n",
    "val_ds = make_dataset(val_pairs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5279446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs source shape: (128, 55)\n",
      "inputs translated shape: (128, 55)\n",
      "target shape: (128, 55)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 22:30:59.136313: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# debugging purpose\n",
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f\"inputs source shape: {inputs['source'].shape}\")\n",
    "    print(f\"inputs translated shape: {inputs['translated'].shape}\")\n",
    "    print(f\"target shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8b05460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input vocabulary:\n",
      "['', '[UNK]', 'a', 'd', 'c', 'b', 'f', 'g', 'e', 'h']\n",
      "output vocabulary:\n",
      "['', '[UNK]', 'd', 'a', 'c', 'b', 'f', 'g', 'e', 'h', 'j', 'i', '[start]', '[end]', 'k', 'l', 'm', 'ed', 'ee', 'ef', 'eg', 'eh']\n"
     ]
    }
   ],
   "source": [
    "in_vocab = source_vectorization.get_vocabulary()\n",
    "out_vocab = target_vectorization.get_vocabulary()\n",
    "print(f\"input vocabulary:\\n{in_vocab}\")\n",
    "print(f\"output vocabulary:\\n{out_vocab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b1a544",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "968b94a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinCosPosEmbed(layers.Layer):\n",
    "    def __init__(self, seq_len, input_dim, output_dim, **kwargs):\n",
    "        super().__init__()\n",
    "        # positional encoding function that maps position of a token to some value represent position\n",
    "        mask_zero = kwargs.get(\"mask_zero\", False)\n",
    "        \n",
    "        self.token_embedding = layers.Embedding(input_dim=input_dim, output_dim=output_dim,\n",
    "                                                mask_zero=mask_zero)\n",
    "        self.position_embedding = layers.Embedding(input_dim=seq_len, output_dim=output_dim,\n",
    "                                                   mask_zero=mask_zero)\n",
    "        self.seq_len = seq_len\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "    \n",
    "    \n",
    "    def getSinCosPositionalEncoding(self, **kwargs):\n",
    "        # positional encoding used by \"attention is all you need\" paper\n",
    "        seq_len = kwargs.get(\"seq_len\")\n",
    "        embed_dim = kwargs.get(\"embed_dim\")\n",
    "        scalar = kwargs.get(\"scalar\", 10000)\n",
    "        positions = np.zeros((seq_len, embed_dim))\n",
    "        for p in range(seq_len):\n",
    "            for i in range(int(embed_dim/2)):\n",
    "                denom = np.power(scalar, 2*i/embed_dim)\n",
    "                positions[p, 2*i] = np.sin(p/denom)\n",
    "                positions[p, 2*i + 1] = np.cos(p/denom)\n",
    "        tf.cast(positions, dtype=tf.float32)\n",
    "        return positions\n",
    "    \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        length = inputs.shape[1]\n",
    "        \n",
    "        # get positional encoding of the input given\n",
    "        arg_dict = {\"inputs\": inputs, \"seq_len\": self.seq_len, \"embed_dim\": self.output_dim}\n",
    "        positions = self.getSinCosPositionalEncoding(**arg_dict)\n",
    "        \n",
    "        embedded_tokens = self.token_embedding(inputs)\n",
    "        # scaling embedded_tokens\n",
    "        embedded_tokens *= tf.math.sqrt(tf.cast(self.output_dim, tf.float32))\n",
    "        positions[tf.newaxis, :length , :]  # reshaping to have the same shape as token embedding\n",
    "        return embedded_tokens + positions\n",
    "    \n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"input_dim\": self.input_dim,\n",
    "            \"seq_len\": self.seq_len,\n",
    "        })\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f5542308",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "global positional_encoding\n",
    "positional_encoding = pe\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, seq_len, input_dim, output_dim, is_pos_embed=False, **kwargs):\n",
    "        super().__init__()\n",
    "        # positional encoding function that maps position of a token to some value represent position\n",
    "        mask_zero = kwargs.get(\"mask_zero\", False)\n",
    "        \n",
    "        self.is_pos_embed = is_pos_embed\n",
    "        self.token_embedding = layers.Embedding(input_dim=input_dim, output_dim=output_dim,\n",
    "                                                mask_zero=mask_zero)\n",
    "        self.position_embedding = layers.Embedding(input_dim=seq_len, output_dim=output_dim,\n",
    "                                                   mask_zero=mask_zero)\n",
    "        self.seq_len = seq_len\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        length = inputs.shape[1]\n",
    "        \n",
    "        arg_dict = {\"inputs\": inputs, \"seq_len\": self.seq_len, \"embed_dim\": self.output_dim}\n",
    "        positions = positional_encoding(**arg_dict)\n",
    "        \n",
    "        embedded_tokens = self.token_embedding(inputs)\n",
    "        # scaling embedded_tokens\n",
    "        embedded_tokens *= tf.math.sqrt(tf.cast(self.output_dim, tf.float32))\n",
    "        if self.is_pos_embed:\n",
    "            positions = self.position_embedding(positions)\n",
    "        else:\n",
    "            positions[tf.newaxis, :length , :]\n",
    "        return embedded_tokens + positions\n",
    "    \n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"input_dim\": self.input_dim,\n",
    "            \"seq_len\": self.seq_len,\n",
    "            \"is_pos_embed\": self.is_pos_embed,\n",
    "        })\n",
    "        return config\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e35fe95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__()\n",
    "        dropout_rate = kwargs.get(\"dropout_rate\",0.0)  # default) no dropout\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        regularizer = kwargs.get(\"regularizer\")  # keras kernel regularizer\n",
    "        \n",
    "        # set up layers for Transformer encoder\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim,\n",
    "                                                   dropout=dropout_rate, kernel_regularizer=regularizer)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim)]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization()\n",
    "        self.layernorm2 = layers.LayerNormalization()\n",
    "    \n",
    "    \n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            # reshaping mask to ensure each batch has 2D mask(axis0: sequence, axis1: embed_dim) \n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        # forward propagation\n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        dense_input = self.layernorm1(inputs + attention_output)\n",
    "        dense_output = self.dense_proj(dense_input)\n",
    "        dense_output = self.dropout(dense_output)\n",
    "        return self.layernorm2(dense_input + dense_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "                \"num_heads\": self.num_heads,\n",
    "                \"dense_dim\": self.dense_dim,\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cf79891",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__()\n",
    "        dropout_rate = kwargs.get(\"dropout_rate\",0.0)  # default) no dropout\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        regularizer = kwargs.get(\"regularizer\")  # keras kernel regularizer\n",
    "        \n",
    "        # set up layers for Transformer decoder\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim,\n",
    "                                                   dropout=dropout_rate, kernel_regularizer=regularizer)\n",
    "        self.attention2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim,\n",
    "                                                   dropout=dropout_rate, kernel_regularizer=regularizer)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim)]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization()\n",
    "        self.layernorm2 = layers.LayerNormalization()\n",
    "        self.layernorm3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "                \"num_heads\": self.num_heads,\n",
    "                \"dense_dim\": self.dense_dim,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "    \n",
    "    \n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        # forward propagate\n",
    "        attention_output1 = self.attention1(query=inputs, value=inputs, key=inputs,\n",
    "                                           use_causal_mask = True)\n",
    "        attention_output1 = self.layernorm1(inputs + attention_output1)\n",
    "        # use encoder outputs as key and value\n",
    "        attention_output2 = self.attention2(\n",
    "            query=attention_output1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "        )\n",
    "        attention_output2 = self.layernorm2(attention_output1 + attention_output2)\n",
    "        output = self.dense_proj(attention_output2)\n",
    "        output = self.dropout(output)\n",
    "        return self.layernorm3(attention_output2 + output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e1ec3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenGenerator():\n",
    "    def __init__(self):\n",
    "        raise NotImplemented\n",
    "    \n",
    "    def __call__(self):\n",
    "        raise NotImplemented\n",
    "\n",
    "class Generate_tokens(TokenGenerator):\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def __call__(self, input_seq, target_vectorization, source_vectorization, model, **kwargs):\n",
    "        max_out_seq_len = kwargs.get(\"max_out_seq_len\", 30)\n",
    "        \n",
    "        out_vocab = target_vectorization.get_vocabulary()\n",
    "        out_index_lookup = dict(zip(range(len(out_vocab)), out_vocab))  # mapping vectorization to vocab\n",
    "        tokenized_input_seq = source_vectorization([input_seq])\n",
    "        decoded_sentence = \"[start]\"\n",
    "        for i in range(max_out_seq_len):\n",
    "            tokenized_out_sentence = target_vectorization([decoded_sentence])[:, :-1]\n",
    "            # input: 1st elem for encoder, and 2nd elem for the first self-attention layer in decoder\n",
    "            predictions = model([tokenized_input_seq, tokenized_out_sentence])\n",
    "            predicted_token_index = np.argmax(predictions[0, i, :])  # a token with highest probabillity\n",
    "            predicted_token = out_index_lookup[predicted_token_index]\n",
    "            decoded_sentence += \" \" + predicted_token\n",
    "            if predicted_token == \"[end]\":\n",
    "                break\n",
    "        return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4181ee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_txt = [pair[0] for pair in test_pairs]\n",
    "test_target_txt = [pair[1] for pair in test_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1093d00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension for Transformer\n",
    "embed_dim = 16\n",
    "dense_dim = 128\n",
    "num_heads = [2,4,8,16, 32, 55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "792850cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttentionLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    # learning rate schedule used in the \"attention is all you need\" paper\n",
    "    def __init__(self, embed_dim, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        elem1 = tf.math.rsqrt(step)\n",
    "        elem2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(tf.cast(self.embed_dim, tf.float32)) * tf.math.minimum(elem1, elem2)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "        }\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6392d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = AttentionLRSchedule(embed_dim)\n",
    "# optimizer setting followed the \"attention is all you need\" paper\n",
    "optim = tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bed36249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(label, pred):\n",
    "    # loss fucntion for machine translation(Cross entropy modified to account for masking)\n",
    "    mask = label != 0\n",
    "    loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    loss = loss_function(label, pred)\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask  # no loss computed for a token that is masked in a sequence\n",
    "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)  # averaged by the number of non-mask tokens\n",
    "    return loss\n",
    "    \n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "    # model accuracy computation accounting for masking\n",
    "    pred = tf.argmax(pred, axis=-1)\n",
    "    label = tf.cast(label, pred.dtype)\n",
    "    accurate = label == pred  # boolean tensor True if predicted correctly\n",
    "    mask = label != 0\n",
    "    accurate = accurate & mask  # mark as accurate if correctly predicted and not masked token\n",
    "    accurate = tf.cast(accurate, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accurate) / tf.reduce_sum(mask)  # averaged by the number of non-mask tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bd76978",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1aa1843c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " source (InputLayer)            [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " sin_cos_pos_embed (SinCosPosEm  (None, 55, 16)      160         ['source[0][0]']                 \n",
      " bed)                                                                                             \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 55, 16)       0           ['sin_cos_pos_embed[0][0]']      \n",
      "                                                                                                  \n",
      " transformer_encoder (Transform  (None, 55, 16)      6464        ['dropout[0][0]']                \n",
      " erEncoder)                                                                                       \n",
      "                                                                                                  \n",
      " translated (InputLayer)        [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " transformer_encoder_1 (Transfo  (None, 55, 16)      8608        ['transformer_encoder[0][0]']    \n",
      " rmerEncoder)                                                                                     \n",
      "                                                                                                  \n",
      " sin_cos_pos_embed_1 (SinCosPos  (None, 55, 16)      352         ['translated[0][0]']             \n",
      " Embed)                                                                                           \n",
      "                                                                                                  \n",
      " transformer_encoder_2 (Transfo  (None, 55, 16)      12896       ['transformer_encoder_1[0][0]']  \n",
      " rmerEncoder)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 55, 16)       0           ['sin_cos_pos_embed_1[0][0]']    \n",
      "                                                                                                  \n",
      " transformer_encoder_3 (Transfo  (None, 55, 16)      21472       ['transformer_encoder_2[0][0]']  \n",
      " rmerEncoder)                                                                                     \n",
      "                                                                                                  \n",
      " transformer_decoder (Transform  (None, 55, 16)      8656        ['dropout_5[0][0]',              \n",
      " erDecoder)                                                       'transformer_encoder_3[0][0]']  \n",
      "                                                                                                  \n",
      " transformer_decoder_1 (Transfo  (None, 55, 16)      12944       ['transformer_decoder[0][0]',    \n",
      " rmerDecoder)                                                     'transformer_encoder_3[0][0]']  \n",
      "                                                                                                  \n",
      " transformer_decoder_2 (Transfo  (None, 55, 16)      21520       ['transformer_decoder_1[0][0]',  \n",
      " rmerDecoder)                                                     'transformer_encoder_3[0][0]']  \n",
      "                                                                                                  \n",
      " transformer_decoder_3 (Transfo  (None, 55, 16)      38672       ['transformer_decoder_2[0][0]',  \n",
      " rmerDecoder)                                                     'transformer_encoder_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 55, 16)       0           ['transformer_decoder_3[0][0]']  \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 55, 22)       374         ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 132,118\n",
      "Trainable params: 132,118\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# multi stacked transformers\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"source\")\n",
    "x = SinCosPosEmbed(input_seq_len, input_vocab_size, embed_dim)(encoder_inputs)\n",
    "x = layers.Dropout(dropout_rate)(x)\n",
    "TE = TransformerEncoder(embed_dim, dense_dim, num_heads[0])(x)\n",
    "TE = TransformerEncoder(embed_dim, dense_dim, num_heads[1])(TE)\n",
    "TE = TransformerEncoder(embed_dim, dense_dim, num_heads[2])(TE)\n",
    "TE = TransformerEncoder(embed_dim, dense_dim, num_heads[3])(TE)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"translated\")\n",
    "x = SinCosPosEmbed(output_seq_len, output_vocab_size, embed_dim)(decoder_inputs)\n",
    "x = layers.Dropout(dropout_rate)(x)\n",
    "TD = TransformerDecoder(embed_dim, dense_dim, num_heads[0])(x, TE)\n",
    "TD = TransformerDecoder(embed_dim, dense_dim, num_heads[1])(TD, TE)\n",
    "TD = TransformerDecoder(embed_dim, dense_dim, num_heads[2])(TD, TE)\n",
    "TD = TransformerDecoder(embed_dim, dense_dim, num_heads[3])(TD, TE)\n",
    "\n",
    "x = layers.Dropout(dropout_rate)(TD)\n",
    "\n",
    "decoder_outputs = layers.Dense(output_vocab_size)(x)\n",
    "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68be292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.compile(loss=masked_loss, optimizer=optim, metrics=[masked_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c295cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "613/613 [==============================] - 310s 499ms/step - loss: 1.3108 - masked_accuracy: 0.3194 - val_loss: 0.6685 - val_masked_accuracy: 0.5824\n",
      "Epoch 2/30\n",
      "613/613 [==============================] - 304s 496ms/step - loss: 0.5683 - masked_accuracy: 0.6268 - val_loss: 0.3790 - val_masked_accuracy: 0.7397\n",
      "Epoch 3/30\n",
      "613/613 [==============================] - 304s 496ms/step - loss: 0.3601 - masked_accuracy: 0.7577 - val_loss: 0.2268 - val_masked_accuracy: 0.8465\n",
      "Epoch 4/30\n",
      "613/613 [==============================] - 304s 496ms/step - loss: 0.2232 - masked_accuracy: 0.8532 - val_loss: 0.1236 - val_masked_accuracy: 0.9215\n",
      "Epoch 5/30\n",
      "613/613 [==============================] - 304s 496ms/step - loss: 0.1315 - masked_accuracy: 0.9174 - val_loss: 0.0587 - val_masked_accuracy: 0.9639\n",
      "Epoch 6/30\n",
      "613/613 [==============================] - 304s 496ms/step - loss: 0.0813 - masked_accuracy: 0.9539 - val_loss: 0.0172 - val_masked_accuracy: 0.9906\n",
      "Epoch 7/30\n",
      "613/613 [==============================] - 311s 507ms/step - loss: 0.0613 - masked_accuracy: 0.9678 - val_loss: 0.0178 - val_masked_accuracy: 0.9909\n",
      "Epoch 8/30\n",
      "613/613 [==============================] - 307s 501ms/step - loss: 0.0358 - masked_accuracy: 0.9818 - val_loss: 0.0080 - val_masked_accuracy: 0.9958\n",
      "Epoch 9/30\n",
      "613/613 [==============================] - 308s 502ms/step - loss: 0.0271 - masked_accuracy: 0.9869 - val_loss: 0.0072 - val_masked_accuracy: 0.9965\n",
      "Epoch 10/30\n",
      "613/613 [==============================] - 312s 509ms/step - loss: 0.0188 - masked_accuracy: 0.9911 - val_loss: 0.0189 - val_masked_accuracy: 0.9914\n",
      "Epoch 11/30\n",
      "613/613 [==============================] - 315s 513ms/step - loss: 0.0167 - masked_accuracy: 0.9923 - val_loss: 0.0031 - val_masked_accuracy: 0.9985\n",
      "Epoch 12/30\n",
      "613/613 [==============================] - 311s 508ms/step - loss: 0.0131 - masked_accuracy: 0.9940 - val_loss: 0.0031 - val_masked_accuracy: 0.9984\n",
      "Epoch 13/30\n",
      "613/613 [==============================] - 307s 501ms/step - loss: 0.0122 - masked_accuracy: 0.9947 - val_loss: 0.0052 - val_masked_accuracy: 0.9972\n",
      "Epoch 14/30\n",
      "613/613 [==============================] - 313s 510ms/step - loss: 0.0117 - masked_accuracy: 0.9947 - val_loss: 0.0025 - val_masked_accuracy: 0.9987\n",
      "Epoch 15/30\n",
      "613/613 [==============================] - 307s 502ms/step - loss: 0.0134 - masked_accuracy: 0.9944 - val_loss: 0.0016 - val_masked_accuracy: 0.9993\n",
      "Epoch 16/30\n",
      "613/613 [==============================] - 304s 497ms/step - loss: 0.0093 - masked_accuracy: 0.9960 - val_loss: 0.0024 - val_masked_accuracy: 0.9987\n",
      "Epoch 17/30\n",
      "613/613 [==============================] - 310s 506ms/step - loss: 0.0096 - masked_accuracy: 0.9958 - val_loss: 0.0053 - val_masked_accuracy: 0.9973\n",
      "Epoch 18/30\n",
      "613/613 [==============================] - 308s 503ms/step - loss: 0.0076 - masked_accuracy: 0.9967 - val_loss: 0.0010 - val_masked_accuracy: 0.9995\n",
      "Epoch 19/30\n",
      "613/613 [==============================] - 306s 499ms/step - loss: 0.0069 - masked_accuracy: 0.9970 - val_loss: 0.0015 - val_masked_accuracy: 0.9993\n",
      "Epoch 20/30\n",
      "613/613 [==============================] - 304s 496ms/step - loss: 0.0058 - masked_accuracy: 0.9975 - val_loss: 0.0028 - val_masked_accuracy: 0.9985\n",
      "Epoch 21/30\n",
      "613/613 [==============================] - 304s 496ms/step - loss: 0.0058 - masked_accuracy: 0.9975 - val_loss: 0.0012 - val_masked_accuracy: 0.9994\n",
      "Epoch 22/30\n",
      "613/613 [==============================] - 304s 496ms/step - loss: 0.0046 - masked_accuracy: 0.9979 - val_loss: 0.0033 - val_masked_accuracy: 0.9985\n",
      "Epoch 23/30\n",
      "613/613 [==============================] - 304s 497ms/step - loss: 0.0043 - masked_accuracy: 0.9981 - val_loss: 0.0012 - val_masked_accuracy: 0.9994\n",
      "Epoch 24/30\n",
      "613/613 [==============================] - 305s 497ms/step - loss: 0.0048 - masked_accuracy: 0.9980 - val_loss: 3.0236e-04 - val_masked_accuracy: 0.9999\n",
      "Epoch 25/30\n",
      "613/613 [==============================] - 304s 496ms/step - loss: 0.0040 - masked_accuracy: 0.9982 - val_loss: 8.4170e-04 - val_masked_accuracy: 0.9996\n",
      "Epoch 26/30\n",
      "613/613 [==============================] - 308s 503ms/step - loss: 0.0038 - masked_accuracy: 0.9984 - val_loss: 0.0012 - val_masked_accuracy: 0.9995\n",
      "Epoch 27/30\n",
      "613/613 [==============================] - 311s 507ms/step - loss: 0.0037 - masked_accuracy: 0.9984 - val_loss: 2.8138e-04 - val_masked_accuracy: 0.9999\n",
      "Epoch 28/30\n",
      "613/613 [==============================] - 311s 507ms/step - loss: 0.0036 - masked_accuracy: 0.9985 - val_loss: 9.5538e-04 - val_masked_accuracy: 0.9996\n",
      "Epoch 29/30\n",
      "613/613 [==============================] - 305s 498ms/step - loss: 0.0030 - masked_accuracy: 0.9987 - val_loss: 0.0011 - val_masked_accuracy: 0.9995\n",
      "Epoch 30/30\n",
      "613/613 [==============================] - 307s 500ms/step - loss: 0.0031 - masked_accuracy: 0.9987 - val_loss: 3.2148e-04 - val_masked_accuracy: 0.9999\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(filepath=\"model/stack4_transformer.h5\", monitor='val_loss',\n",
    "                                    save_best_only=True),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=9, verbose=1),\n",
    "]\n",
    "history = transformer.fit(train_ds,\n",
    "                          epochs=30,\n",
    "                          validation_data=val_ds,\n",
    "                          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c22ff685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "train_loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "epochs = range(len(train_loss))\n",
    "plt.plot(epochs, train_loss, label=\"train_loss\")\n",
    "plt.plot(epochs, val_loss, label=\"validation_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9737745a-5fa2-4817-9b6f-d3f056f1bfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = keras.models.load_model(\"model/stack4_transformer.h5\",\n",
    "                                     custom_objects={\n",
    "                                         \"SinCosPosEmbed\": SinCosPosEmbed,\n",
    "                                         \"TransformerEncoder\": TransformerEncoder,\n",
    "                                         \"TransformerDecoder\": TransformerDecoder,\n",
    "                                         \"AttentionLRSchedule\": AttentionLRSchedule,\n",
    "                                         \"masked_loss\": masked_loss,\n",
    "                                         \"masked_accuracy\": masked_accuracy,\n",
    "                                     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "660d07da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " source (InputLayer)            [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " sin_cos_pos_embed_2 (SinCosPos  (None, 55, 16)      160         ['source[0][0]']                 \n",
      " Embed)                                                                                           \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 55, 16)       0           ['sin_cos_pos_embed_2[0][0]']    \n",
      "                                                                                                  \n",
      " transformer_encoder_4 (Transfo  (None, 55, 16)      6464        ['dropout[0][0]']                \n",
      " rmerEncoder)                                                                                     \n",
      "                                                                                                  \n",
      " translated (InputLayer)        [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " transformer_encoder_5 (Transfo  (None, 55, 16)      8608        ['transformer_encoder_4[0][0]']  \n",
      " rmerEncoder)                                                                                     \n",
      "                                                                                                  \n",
      " sin_cos_pos_embed_3 (SinCosPos  (None, 55, 16)      352         ['translated[0][0]']             \n",
      " Embed)                                                                                           \n",
      "                                                                                                  \n",
      " transformer_encoder_6 (Transfo  (None, 55, 16)      12896       ['transformer_encoder_5[0][0]']  \n",
      " rmerEncoder)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 55, 16)       0           ['sin_cos_pos_embed_3[0][0]']    \n",
      "                                                                                                  \n",
      " transformer_encoder_7 (Transfo  (None, 55, 16)      21472       ['transformer_encoder_6[0][0]']  \n",
      " rmerEncoder)                                                                                     \n",
      "                                                                                                  \n",
      " transformer_decoder_4 (Transfo  (None, 55, 16)      8656        ['dropout_5[0][0]',              \n",
      " rmerDecoder)                                                     'transformer_encoder_7[0][0]']  \n",
      "                                                                                                  \n",
      " transformer_decoder_5 (Transfo  (None, 55, 16)      12944       ['transformer_decoder_4[0][0]',  \n",
      " rmerDecoder)                                                     'transformer_encoder_7[0][0]']  \n",
      "                                                                                                  \n",
      " transformer_decoder_6 (Transfo  (None, 55, 16)      21520       ['transformer_decoder_5[0][0]',  \n",
      " rmerDecoder)                                                     'transformer_encoder_7[0][0]']  \n",
      "                                                                                                  \n",
      " transformer_decoder_7 (Transfo  (None, 55, 16)      38672       ['transformer_decoder_6[0][0]',  \n",
      " rmerDecoder)                                                     'transformer_encoder_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 55, 16)       0           ['transformer_decoder_7[0][0]']  \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 55, 22)       374         ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 55, 22)       0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 132,118\n",
      "Trainable params: 132,118\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "t = layers.Concatenate([transformer, layers.Activation(\"softmax\")])\n",
    "inputs = transformer.inputs\n",
    "out = transformer.outputs[0]\n",
    "outputs = layers.Activation(\"softmax\")(out)\n",
    "t = keras.Model(inputs=inputs, outputs=outputs)\n",
    "t.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de6d96dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Input Text Sequences: 16800\n",
      "Currently at sequence 0, and count: 0\n",
      "Model Tranlsated: [start] c d b d c g a f e f b d a h d g h c e b d a d j k a f i l [end]\n",
      "Ground-Truth:     [start] c d b d c g a f e f b d a h d g h c e b d a d j k a f i l [end]\n",
      "Currently at sequence 100, and count: 100\n",
      "Currently at sequence 200, and count: 200\n",
      "Model Tranlsated: [start] b d b d b d a e f a h d e g b d b d b d a h i j k a f h l [end]\n",
      "Ground-Truth:     [start] b d b d b d a e f a h d e g b d b d b d a h i j k a f h l [end]\n",
      "Currently at sequence 300, and count: 300\n",
      "Currently at sequence 400, and count: 400\n",
      "Currently at sequence 500, and count: 499\n",
      "Model Tranlsated: [start] c e c d a g d e a e f c e a d g h b d b d c g a h j k l b d a g m ed a d i ee [end]\n",
      "Ground-Truth:     [start] c e c d a g d e a e f c e a d g h b d b d c g a h j k l b d a g m ed a d i ee [end]\n",
      "Currently at sequence 600, and count: 598\n",
      "Model Tranlsated: [start] b d b d b d a g e f a f d g b d a e i a f h j [end]\n",
      "Ground-Truth:     [start] b d b d b d a g e f a f d g b d a e i a f h j [end]\n",
      "Currently at sequence 700, and count: 697\n",
      "Model Tranlsated: [start] c e c g c f a h d e f b d b d a g h i a f g j a e k [end]\n",
      "Ground-Truth:     [start] c e c g c f a h d e f b d b d a g h i a f g j a e k [end]\n",
      "Model Tranlsated: [start] b d c e c f a f e f a f d g c f b d c e a f j k a d i l a f h m [end]\n",
      "Ground-Truth:     [start] b d c e c f a f e f a f d g c f b d c e a f j k a d i l a f h m [end]\n",
      "Currently at sequence 800, and count: 797\n",
      "Model Tranlsated: [start] b d b d a d d e c d a d f g a e h b d b d b d a h j k l b d a e ed a h i m ee [end]\n",
      "Ground-Truth:     [start] b d b d a d d e c d a d f g a e h b d b d b d a h j k l b d a e ed a h i m ee [end]\n",
      "Currently at sequence 900, and count: 897\n",
      "Currently at sequence 1000, and count: 996\n",
      "Currently at sequence 1100, and count: 1096\n",
      "Currently at sequence 1200, and count: 1196\n",
      "Model Tranlsated: [start] b d b d b d c d b d a h f g h a d e i c g a h d j k c e a g l m [end]\n",
      "Ground-Truth:     [start] b d b d b d c d b d a h f g h a d e i c g a h d j k c e a g l m [end]\n",
      "Model Tranlsated: [start] c g c d b d a f e f b d a f g h b d a h d i j [end]\n",
      "Ground-Truth:     [start] c g c d b d a f e f b d a f g h b d a h d i j [end]\n",
      "Model Tranlsated: [start] c e b d b d b d b d c g c g a h h i j b d a e l a h g k m a h e f ed a e ee a f d ef [end]\n",
      "Ground-Truth:     [start] c e b d b d b d b d c g c g a h h i j b d a e l a h g k m a h e f ed a e ee a f d ef [end]\n",
      "Currently at sequence 1300, and count: 1296\n",
      "Currently at sequence 1400, and count: 1396\n",
      "Currently at sequence 1500, and count: 1494\n",
      "Currently at sequence 1600, and count: 1593\n",
      "Currently at sequence 1700, and count: 1691\n",
      "Model Tranlsated: [start] c f c g b d a h d e f b d c g b d c e a g j k a h h i l a g g m [end]\n",
      "Ground-Truth:     [start] c f c g b d a h d e f b d c g b d c e a g j k a h h i l a g g m [end]\n",
      "Currently at sequence 1800, and count: 1791\n",
      "Currently at sequence 1900, and count: 1890\n",
      "Currently at sequence 2000, and count: 1990\n",
      "Currently at sequence 2100, and count: 2090\n",
      "Currently at sequence 2200, and count: 2190\n",
      "Currently at sequence 2300, and count: 2290\n",
      "Model Tranlsated: [start] b d c f c e a e f a h d e g a e h c g a g i j b d a f k l [end]\n",
      "Ground-Truth:     [start] b d c f c e a e f a h d e g a e h c g a g i j b d a f k l [end]\n",
      "Currently at sequence 2400, and count: 2390\n",
      "Currently at sequence 2500, and count: 2490\n",
      "Currently at sequence 2600, and count: 2590\n",
      "Model Tranlsated: [start] b d c g a d d e a e f b d a g g h c e b d a d j k a g i l c e a f m ed [end]\n",
      "Ground-Truth:     [start] b d c g a d d e a e f b d a g g h c e b d a d j k a g i l c e a f m ed [end]\n",
      "Model Tranlsated: [start] b d c g b d c e a e g a d f h c g a d i j a e k c g c g a g m ed a g l ee a f e ef a d d eg [end]\n",
      "Ground-Truth:     [start] b d c g b d c e a e g a d f h c g a d i j a e k c g c g a g m ed a g l ee a f e ef a d d eg [end]\n",
      "Currently at sequence 2700, and count: 2690\n",
      "Model Tranlsated: [start] c e b d b d a f e f a d d g b d b d a f i j a g h k [end]\n",
      "Ground-Truth:     [start] c e b d b d a f e f a d d g b d b d a f i j a g h k [end]\n",
      "Currently at sequence 2800, and count: 2790\n",
      "Model Tranlsated: [start] c e c d c g a d e f c d b d a g h i a g g j a d d k [end]\n",
      "Ground-Truth:     [start] c e c d c g a d e f c d b d a g h i a g g j a d d k [end]\n",
      "Currently at sequence 2900, and count: 2890\n",
      "Currently at sequence 3000, and count: 2989\n",
      "Currently at sequence 3100, and count: 3089\n",
      "Currently at sequence 3200, and count: 3189\n",
      "Currently at sequence 3300, and count: 3289\n",
      "Model Tranlsated: [start] c f c e b d b d a d f g b d a h e h i b d a h d j k [end]\n",
      "Ground-Truth:     [start] c f c e b d b d a d f g b d a h e h i b d a h d j k [end]\n",
      "Currently at sequence 3400, and count: 3389\n",
      "Currently at sequence 3500, and count: 3488\n",
      "Currently at sequence 3600, and count: 3588\n",
      "Currently at sequence 3700, and count: 3688\n",
      "Model Tranlsated: [start] b d b d c e a g e f a d d g b d a d h i a e j [end]\n",
      "Ground-Truth:     [start] b d b d c e a g e f a d d g b d a d h i a e j [end]\n",
      "Model Tranlsated: [start] c d c e a f d e b d a f f g c d c g a h h i j [end]\n",
      "Ground-Truth:     [start] c d c e a f d e b d a f f g c d c g a h h i j [end]\n",
      "Currently at sequence 3800, and count: 3788\n",
      "Currently at sequence 3900, and count: 3887\n",
      "Model Tranlsated: [start] b d c f b d a d e f b d a f g h b d a g i j a g d k [end]\n",
      "Ground-Truth:     [start] b d c f b d a d e f b d a f g h b d a g i j a g d k [end]\n",
      "Model Tranlsated: [start] c d c d c e b d b d c e c e c d a h i j k a g h l b d a h g m ed a h e f ee a g d ef [end]\n",
      "Ground-Truth:     [start] c d c d c e b d b d c e c e c d a h i j k a g h l b d a h g m ed a h e f ee a g d ef [end]\n",
      "Currently at sequence 4000, and count: 3987\n",
      "Model Tranlsated: [start] b d c g b d b d a g f g a e h a h d e i c e a d j k a e l [end]\n",
      "Ground-Truth:     [start] b d c g b d b d a g f g a e h a h d e i c e a d j k a e l [end]\n",
      "Currently at sequence 4100, and count: 4085\n",
      "Model Tranlsated: [start] b d b d a e e a d d f b d c g a d h i a f g j [end]\n",
      "Ground-Truth:     [start] b d b d a e e a d d f b d c g a d h i a f g j [end]\n",
      "Currently at sequence 4200, and count: 4185\n",
      "Currently at sequence 4300, and count: 4285\n",
      "Currently at sequence 4400, and count: 4385\n",
      "Currently at sequence 4500, and count: 4484\n",
      "Currently at sequence 4600, and count: 4583\n",
      "Model Tranlsated: [start] b d b d a d d e b d a d f g c f a e i a g h j b d c d a h k l m [end]\n",
      "Ground-Truth:     [start] b d b d a d d e b d a d f g c f a e i a g h j b d c d a h k l m [end]\n",
      "Currently at sequence 4700, and count: 4682\n",
      "Currently at sequence 4800, and count: 4782\n",
      "Currently at sequence 4900, and count: 4882\n",
      "Model Tranlsated: [start] b d a e d c g c f b d a e h a g g i a h e f j [end]\n",
      "Ground-Truth:     [start] b d a e d c g c f b d a e h a g g i a h e f j [end]\n",
      "Model Tranlsated: [start] b d b d a e e b d a h d f g b d c d a g i j c d a h h k l b d c g a h m ed ee [end]\n",
      "Ground-Truth:     [start] b d b d a e e b d a h d f g b d c d a g i j c d a h h k l b d c g a h m ed ee [end]\n",
      "Currently at sequence 5000, and count: 4982\n",
      "Currently at sequence 5100, and count: 5082\n",
      "Model Tranlsated: [start] b d c g a d d e c g a g f g c g c f a h h i j [end]\n",
      "Ground-Truth:     [start] b d c g a d d e c g a g f g c g c f a h h i j [end]\n",
      "Currently at sequence 5200, and count: 5181\n",
      "Model Tranlsated: [start] b d c g c g c f c d a g g h a e i c g a h f j k a h d e l a e m [end]\n",
      "Ground-Truth:     [start] b d c g c g c f c d a g g h a e i c g a h f j k a h d e l a e m [end]\n",
      "Currently at sequence 5300, and count: 5280\n",
      "Currently at sequence 5400, and count: 5380\n",
      "Model Tranlsated: [start] c f c g c e c f a h e f g a e h b d b d a h i j k a d d l a e m [end]\n",
      "Ground-Truth:     [start] c f c g c e c f a h e f g a e h b d b d a h i j k a d d l a e m [end]\n",
      "Currently at sequence 5500, and count: 5480\n",
      "Currently at sequence 5600, and count: 5580\n",
      "Currently at sequence 5700, and count: 5680\n",
      "Currently at sequence 5800, and count: 5780\n",
      "Model Tranlsated: [start] b d b d b d b d a h e f g a d d h b d a f i j [end]\n",
      "Ground-Truth:     [start] b d b d b d b d a h e f g a d d h b d a f i j [end]\n",
      "Currently at sequence 5900, and count: 5880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently at sequence 6000, and count: 5980\n",
      "Model Tranlsated: [start] b d c e a d d e a e f c d b d b d a f i j c e c d a d l m a g k ed a g h ee b d a f ef eg a f g eh [end]\n",
      "Ground-Truth:     [start] b d c e a d d e a e f c d b d b d a f i j c e c d a d l m a g k ed a g h ee b d a f ef eg a f g eh [end]\n",
      "Currently at sequence 6100, and count: 6080\n",
      "Model Tranlsated: [start] b d b d a g d e c d c e c g a d h i c e b d a h j k l a h f g m [end]\n",
      "Ground-Truth:     [start] b d b d a g d e c d c e c g a d h i c e b d a h j k l a h f g m [end]\n",
      "Model Tranlsated: [start] b d b d a d d e b d a d f g a e h c f c g a e k a e l a h i j m [end]\n",
      "Ground-Truth:     [start] b d b d a d d e b d a d f g a e h c f c g a e k a e l a h i j m [end]\n",
      "Currently at sequence 6200, and count: 6179\n",
      "Model Tranlsated: [start] c g b d c e a h d e f c g c e a d h i a e j a g g k c e c d a g m ed b d a h l ee ef [end]\n",
      "Ground-Truth:     [start] c g b d c e a h d e f c g c e a d h i a e j a g g k c e c d a g m ed b d a h l ee ef [end]\n",
      "Currently at sequence 6300, and count: 6279\n",
      "Currently at sequence 6400, and count: 6378\n",
      "Model Tranlsated: [start] c g c d b d c d a f f g a f e h b d a h d i j c e a d k l [end]\n",
      "Ground-Truth:     [start] c g c d b d c d a f f g a f e h b d a h d i j c e a d k l [end]\n",
      "Currently at sequence 6500, and count: 6477\n",
      "Model Tranlsated: [start] b d c g a e e b d a f f g a e h c e a f i j b d a d k l b d a h d m ed a e ee a e ef c e a g eg eh [end]\n",
      "Ground-Truth:     [start] b d c g a e e b d a f f g a e h c e a f i j b d a d k l b d a h d m ed a e ee a e ef c e a g eg eh [end]\n",
      "Currently at sequence 6600, and count: 6576\n",
      "Model Tranlsated: [start] b d b d b d c f a f f g a e h a d e i a g d j [end]\n",
      "Ground-Truth:     [start] b d b d b d c f a f f g a e h a d e i a g d j [end]\n",
      "Model Tranlsated: [start] b d b d c g c g c d a h f g h a f e i a f d j [end]\n",
      "Ground-Truth:     [start] b d b d c g c g c d a h f g h a f e i a f d j [end]\n",
      "Currently at sequence 6700, and count: 6675\n",
      "Model Tranlsated: [start] c g b d b d a d e f a f d g b d a e i c d a d j k a d h l [end]\n",
      "Ground-Truth:     [start] c g b d b d a d e f a f d g b d a e i c d a d j k a d h l [end]\n",
      "Currently at sequence 6800, and count: 6775\n",
      "Model Tranlsated: [start] c g c d b d a h d e f c d b d c f b d a d j k c e a g l m a h h i ed a d g ee [end]\n",
      "Ground-Truth:     [start] c g c d b d a h d e f c d b d c f b d a d j k c e a g l m a h h i ed a d g ee [end]\n",
      "Currently at sequence 6900, and count: 6874\n",
      "Model Tranlsated: [start] c f a e d b d a g e f c g c d a f h i a g g j [end]\n",
      "Ground-Truth:     [start] c f a e d b d a g e f c g c d a f h i a g g j [end]\n",
      "Currently at sequence 7000, and count: 6974\n",
      "Model Tranlsated: [start] b d c e b d a e f c e b d a h g h i a h d e j [end]\n",
      "Ground-Truth:     [start] b d c e b d a e f c e b d a h g h i a h d e j [end]\n",
      "Currently at sequence 7100, and count: 7074\n",
      "Model Tranlsated: [start] b d c e a d d e c f a e g c d a g h i a d f j [end]\n",
      "Ground-Truth:     [start] b d c e a d d e c f a e g c d a g h i a d f j [end]\n",
      "Currently at sequence 7200, and count: 7173\n",
      "Model Tranlsated: [start] c g c d a d d e a e f b d c g a g h i a d g j a e k [end]\n",
      "Ground-Truth:     [start] c g c d a d d e a e f b d c g a g h i a d g j a e k [end]\n",
      "Model Tranlsated: [start] b d c d c d a e f a e g a d e h c e b d c g a h j k l a g i m a d d ed [end]\n",
      "Ground-Truth:     [start] b d c d c d a e f a e g a d e h c e b d c g a h j k l a g i m a d d ed [end]\n",
      "Currently at sequence 7300, and count: 7272\n",
      "Model Tranlsated: [start] b d b d a f d e b d a g f g b d b d a d i j a g h k [end]\n",
      "Ground-Truth:     [start] b d b d a f d e b d a g f g b d b d a d i j a g h k [end]\n",
      "Currently at sequence 7400, and count: 7372\n",
      "Model Tranlsated: [start] b d b d b d a d e f b d b d a h g h i a d d j c e a g k l b d a f m ed [end]\n",
      "Ground-Truth:     [start] b d b d b d a d e f b d b d a h g h i a d d j c e a g k l b d a f m ed [end]\n",
      "Model Tranlsated: [start] b d b d a g d e c f c e c d b d c d a f j k a h h i l a d g m a d f ed [end]\n",
      "Ground-Truth:     [start] b d b d a g d e c f c e c d b d c d a f j k a h h i l a d g m a d f ed [end]\n",
      "Model Tranlsated: [start] c d c d c f c e a h e f g c d a e i a f h j a e k a d d l [end]\n",
      "Ground-Truth:     [start] c d c d c f c e a h e f g c d a e i a f h j a e k a d d l [end]\n",
      "Currently at sequence 7500, and count: 7471\n",
      "Currently at sequence 7600, and count: 7571\n",
      "Model Tranlsated: [start] b d b d a d d e c e a f f g c d a f h i a e j [end]\n",
      "Ground-Truth:     [start] b d b d a d d e c e a f f g c d a f h i a e j [end]\n",
      "Currently at sequence 7700, and count: 7671\n",
      "Currently at sequence 7800, and count: 7770\n",
      "Model Tranlsated: [start] c d b d b d a h d e f b d a d g h c d a g i j b d a g k l [end]\n",
      "Ground-Truth:     [start] c d b d b d a h d e f b d a d g h c d a g i j b d a g k l [end]\n",
      "Currently at sequence 7900, and count: 7869\n",
      "Model Tranlsated: [start] c f b d b d b d b d b d a h g h i a h e f j a g d k [end]\n",
      "Ground-Truth:     [start] c f b d b d b d b d b d a h g h i a h e f j a g d k [end]\n",
      "Currently at sequence 8000, and count: 7968\n",
      "Currently at sequence 8100, and count: 8068\n",
      "Currently at sequence 8200, and count: 8168\n",
      "Currently at sequence 8300, and count: 8268\n",
      "Model Tranlsated: [start] c e b d b d c f a g f g c e b d a f i j a d h k c d c g a g m ed a g l ee c e a h e ef eg a g d eh [end]\n",
      "Ground-Truth:     [start] c e b d b d c f a g f g c e b d a f i j a d h k c d c g a g m ed a g l ee c e a h e ef eg a g d eh [end]\n",
      "Currently at sequence 8400, and count: 8368\n",
      "Currently at sequence 8500, and count: 8467\n",
      "Model Tranlsated: [start] b d c g b d a g e f a f d g a e h c e a g i j [end]\n",
      "Ground-Truth:     [start] b d c g b d a g e f a f d g a e h c e a g i j [end]\n",
      "Currently at sequence 8600, and count: 8565\n",
      "Model Tranlsated: [start] c d c g b d a e f b d b d a g h i a g g j a g e k a d d l [end]\n",
      "Ground-Truth:     [start] c d c g b d a e f b d b d a g h i a g g j a g e k a d d l [end]\n",
      "Currently at sequence 8700, and count: 8664\n",
      "Currently at sequence 8800, and count: 8764\n",
      "Model Tranlsated: [start] b d c e b d a h d e f c e c d b d a e j c f a h i k l b d a h h m ed c g a d ee ef a g g eg [end]\n",
      "Ground-Truth:     [start] b d c e b d a h d e f c e c d b d a e j c f a h i k l b d a h h m ed c g a d ee ef a g g eg [end]\n",
      "Currently at sequence 8900, and count: 8863\n",
      "Model Tranlsated: [start] c g b d c f a f e f a e g c g a f h i c d c d a f k l a e m a e ed b d a h j ee ef a f d eg [end]\n",
      "Ground-Truth:     [start] c g b d c f a f e f a e g c g a f h i c d c d a f k l a e m a e ed b d a h j ee ef a f d eg [end]\n",
      "Currently at sequence 9000, and count: 8963\n",
      "Model Tranlsated: [start] c g c g c e c d a h e f g a g d h b d a d i j [end]\n",
      "Ground-Truth:     [start] c g c g c e c d a h e f g a g d h b d a d i j [end]\n",
      "Model Tranlsated: [start] c g c f b d c e c g a d g h a h e f i b d c g a d k l a e m a d j ed a d d ee [end]\n",
      "Ground-Truth:     [start] c g c f b d c e c g a d g h a h e f i b d c g a d k l a e m a d j ed a d d ee [end]\n",
      "Currently at sequence 9100, and count: 9063\n",
      "Currently at sequence 9200, and count: 9163\n",
      "Currently at sequence 9300, and count: 9262\n",
      "Model Tranlsated: [start] b d b d c g b d a f f g c e c e a d i j c d c e a f l m a f k ed b d a g ee ef a h e h eg a g d eh [end]\n",
      "Ground-Truth:     [start] b d b d c g b d a f f g c e c e a d i j c d c e a f l m a f k ed b d a g ee ef a h e h eg a g d eh [end]\n",
      "Model Tranlsated: [start] c e c g a e e a d d f b d a g g h b d a g i j [end]\n",
      "Ground-Truth:     [start] c e c g a e e a d d f b d a g g h b d a g i j [end]\n",
      "Currently at sequence 9400, and count: 9361\n",
      "Model Tranlsated: [start] c g c f b d a g e f a e g b d c f a e j c e a d k l a h h i m a f d ed b d a d ee ef [end]\n",
      "Ground-Truth:     [start] c g c f b d a g e f a e g b d c f a e j c e a d k l a h h i m a f d ed b d a d ee ef [end]\n",
      "Currently at sequence 9500, and count: 9460\n",
      "Currently at sequence 9600, and count: 9559\n",
      "Currently at sequence 9700, and count: 9657\n",
      "Currently at sequence 9800, and count: 9757\n",
      "Currently at sequence 9900, and count: 9855\n",
      "Currently at sequence 10000, and count: 9954\n",
      "Model Tranlsated: [start] b d b d c g a h d e f c g b d a g h i a d g j [end]\n",
      "Ground-Truth:     [start] b d b d c g a h d e f c g b d a g h i a d g j [end]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently at sequence 10100, and count: 10054\n",
      "Currently at sequence 10200, and count: 10154\n",
      "Model Tranlsated: [start] c g b d a f d e b d a g f g c e b d a h h i j [end]\n",
      "Ground-Truth:     [start] c g b d a f d e b d a g f g c e b d a h h i j [end]\n",
      "Currently at sequence 10300, and count: 10254\n",
      "Currently at sequence 10400, and count: 10353\n",
      "Currently at sequence 10500, and count: 10451\n",
      "Currently at sequence 10600, and count: 10551\n",
      "Model Tranlsated: [start] b d c d a d d e b d b d a g g h c g a d i j a f f k a e l [end]\n",
      "Ground-Truth:     [start] b d c d a d d e b d b d a g g h c g a d i j a f f k a e l [end]\n",
      "Currently at sequence 10700, and count: 10651\n",
      "Currently at sequence 10800, and count: 10751\n",
      "Model Tranlsated: [start] b d a e d b d c f a h e f g b d a f h i c g c e a f k l a e m a d j ed [end]\n",
      "Ground-Truth:     [start] b d a e d b d c f a h e f g b d a f h i c g c e a f k l a e m a d j ed [end]\n",
      "Currently at sequence 10900, and count: 10850\n",
      "Model Tranlsated: [start] b d b d c f c e a e g a h e f h a g d i a e j c d a e l c f c e a g ed ee a f m ef a g k eg [end]\n",
      "Ground-Truth:     [start] b d b d c f c e a e g a h e f h a g d i a e j c d a e l c f c e a g ed ee a f m ef a g k eg [end]\n",
      "Currently at sequence 11000, and count: 10949\n",
      "Model Tranlsated: [start] c e b d b d b d b d b d c d a f i j a g h k a d g l c f a g m ed a h e f ee a g d ef [end]\n",
      "Ground-Truth:     [start] c e b d b d b d b d b d c d a f i j a g h k a d g l c f a g m ed a h e f ee a g d ef [end]\n",
      "Currently at sequence 11100, and count: 11049\n",
      "Currently at sequence 11200, and count: 11149\n",
      "Model Tranlsated: [start] c d c d b d a h d e f b d b d a d h i a f g j [end]\n",
      "Ground-Truth:     [start] c d c d b d a h d e f b d b d a d h i a f g j [end]\n",
      "Currently at sequence 11300, and count: 11246\n",
      "Currently at sequence 11400, and count: 11346\n",
      "Currently at sequence 11500, and count: 11446\n",
      "Currently at sequence 11600, and count: 11546\n",
      "Currently at sequence 11700, and count: 11646\n",
      "Currently at sequence 11800, and count: 11745\n",
      "Model Tranlsated: [start] b d c f b d b d a e g a f f h a f e i b d a h d j k b d a g l m b d a g ed ee [end]\n",
      "Ground-Truth:     [start] b d c f b d b d a e g a f f h a f e i b d a h d j k b d a g l m b d a g ed ee [end]\n",
      "Currently at sequence 11900, and count: 11845\n",
      "Currently at sequence 12000, and count: 11945\n",
      "Model Tranlsated: [start] c f b d b d a h d e f c e a e h b d a h g i j [end]\n",
      "Ground-Truth:     [start] c f b d b d a h d e f c e a e h b d a h g i j [end]\n",
      "Model Tranlsated: [start] b d b d c e a d e f c d c g c e a h h i j a f g k a f d l [end]\n",
      "Ground-Truth:     [start] b d b d c e a d e f c d c g c e a h h i j a f g k a f d l [end]\n",
      "Model Tranlsated: [start] c e c g b d b d b d c f b d a d i j a e k a g h l a d g m a d f ed a f e ee a d d ef [end]\n",
      "Ground-Truth:     [start] c e c g b d b d b d c f b d a d i j a e k a g h l a d g m a d f ed a f e ee a d d ef [end]\n",
      "Currently at sequence 12100, and count: 12045\n",
      "Currently at sequence 12200, and count: 12145\n",
      "Currently at sequence 12300, and count: 12245\n",
      "Currently at sequence 12400, and count: 12345\n",
      "Currently at sequence 12500, and count: 12445\n",
      "Currently at sequence 12600, and count: 12545\n",
      "Model Tranlsated: [start] c g c e a f d e b d a d f g b d a e i b d a h h j k [end]\n",
      "Ground-Truth:     [start] c g c e a f d e b d a d f g b d a e i b d a h h j k [end]\n",
      "Currently at sequence 12700, and count: 12645\n",
      "Model Tranlsated: [start] c f c e b d b d a h e f g a g d h c d a e j b d a h i k l [end]\n",
      "Ground-Truth:     [start] c f c e b d b d a h e f g a g d h c d a e j b d a h i k l [end]\n",
      "Model Tranlsated: [start] c d b d c g a d e f a e g b d a h d h i c e a d j k b d c e c g a d ed ee a e ef a d m eg a g l eh [end]\n",
      "Ground-Truth:     [start] c d b d c g a d e f a e g b d a h d h i c e a d j k b d c e c g a d ed ee a e ef a d m eg a g l eh [end]\n",
      "Model Tranlsated: [start] b d c d a d d e b d a f f g a e h b d a d i j [end]\n",
      "Ground-Truth:     [start] b d c d a d d e b d a f f g a e h b d a d i j [end]\n",
      "Currently at sequence 12800, and count: 12744\n",
      "Model Tranlsated: [start] c g c e b d b d c d a h f g h c e c g a f j k a h e i l a d d m [end]\n",
      "Ground-Truth:     [start] c g c e b d b d c d a h f g h c e c g a f j k a h e i l a d d m [end]\n",
      "Currently at sequence 12900, and count: 12844\n",
      "Currently at sequence 13000, and count: 12944\n",
      "Model Tranlsated: [start] c g c g a e e c f b d a f g h c f a e j a d i k a d f l a e m a d d ed b d a e ef a f ee eg [end]\n",
      "Ground-Truth:     [start] c g c g a e e c f b d a f g h c f a e j a d i k a d f l a e m a d d ed b d a e ef a f ee eg [end]\n",
      "Currently at sequence 13100, and count: 13044\n",
      "Currently at sequence 13200, and count: 13144\n",
      "Currently at sequence 13300, and count: 13244\n",
      "Model Tranlsated: [start] b d b d c e a e f a e g c e a d h i a h d e j b d a f k l b d a d m ed [end]\n",
      "Ground-Truth:     [start] b d b d c e a e f a e g c e a d h i a h d e j b d a f k l b d a d m ed [end]\n",
      "Currently at sequence 13400, and count: 13342\n",
      "Currently at sequence 13500, and count: 13441\n",
      "Currently at sequence 13600, and count: 13541\n",
      "Currently at sequence 13700, and count: 13640\n",
      "Currently at sequence 13800, and count: 13740\n",
      "Model Tranlsated: [start] b d c d a f d e c e a g f g b d b d b d c e a g k l a g j m a e ed b d a d ee ef a h h i eg [end]\n",
      "Ground-Truth:     [start] b d c d a f d e c e a g f g b d b d b d c e a g k l a g j m a e ed b d a d ee ef a h h i eg [end]\n",
      "Currently at sequence 13900, and count: 13840\n",
      "Model Tranlsated: [start] c d b d c d a d e f a e g c d a h d h i c f a e k a d j l [end]\n",
      "Ground-Truth:     [start] c d b d c d a d e f a e g c d a h d h i c f a e k a d j l [end]\n",
      "Model Tranlsated: [start] c g c g a g d e b d c g c g c e c e a f j k a e l a f i m c e a e ee a h h ed ef c e a h g eg eh a h e eh [end]\n",
      "Ground-Truth:     [start] c g c g a g d e b d c g c g c e c e a f j k a e l a e m a f i ed c e a e ef a h h ee eg a h f g eh [end]\n",
      "Currently at sequence 14000, and count: 13939\n",
      "Currently at sequence 14100, and count: 14039\n",
      "Model Tranlsated: [start] b d c f b d a e f b d a e h b d b d b d a f k l c f a d m ed a h i j ee a d g ef a h d e eg [end]\n",
      "Ground-Truth:     [start] b d c f b d a e f b d a e h b d b d b d a f k l c f a d m ed a h i j ee a d g ef a h d e eg [end]\n",
      "Model Tranlsated: [start] c e b d b d a f e f b d a d g h b d a h d i j [end]\n",
      "Ground-Truth:     [start] c e b d b d a f e f b d a d g h b d a h d i j [end]\n",
      "Currently at sequence 14200, and count: 14138\n",
      "Currently at sequence 14300, and count: 14238\n",
      "Model Tranlsated: [start] c f c e b d c e a h e f g b d a h d h i b d a d j k b d b d a h l m ed [end]\n",
      "Ground-Truth:     [start] c f c e b d c e a h e f g b d a h d h i b d a d j k b d b d a h l m ed [end]\n",
      "Currently at sequence 14400, and count: 14337\n",
      "Model Tranlsated: [start] c e b d b d b d a e g c e b d a h h i j b d a e l b d a f m ed a h f k ee a h d e ef [end]\n",
      "Ground-Truth:     [start] c e b d b d b d a e g c e b d a h h i j b d a e l b d a f m ed a h f k ee a h d e ef [end]\n",
      "Currently at sequence 14500, and count: 14436\n",
      "Model Tranlsated: [start] b d a e d c g b d a g f g b d a g h i a d e j [end]\n",
      "Ground-Truth:     [start] b d a e d c g b d a g f g b d a g h i a d e j [end]\n",
      "Currently at sequence 14600, and count: 14536\n",
      "Model Tranlsated: [start] c g a e d c e b d b d a g g h a g f i a d e j [end]\n",
      "Ground-Truth:     [start] c g a e d c e b d b d a g g h a g f i a d e j [end]\n",
      "Currently at sequence 14700, and count: 14635\n",
      "Currently at sequence 14800, and count: 14733\n",
      "Model Tranlsated: [start] c e a e d b d c d a e g a d f h c f b d a g j k a h e i l b d a e ed a d m ee [end]\n",
      "Ground-Truth:     [start] c e a e d b d c d a e g a d f h c f b d a g j k a h e i l b d a e ed a d m ee [end]\n",
      "Currently at sequence 14900, and count: 14833\n",
      "Currently at sequence 15000, and count: 14933\n",
      "Currently at sequence 15100, and count: 15033\n",
      "Model Tranlsated: [start] b d a e d b d a f e f b d c g a f h i b d c d a e l a h j k m c e a h g ed ee [end]\n",
      "Ground-Truth:     [start] b d a e d b d a f e f b d c g a f h i b d c d a e l a h j k m c e a h g ed ee [end]\n",
      "Currently at sequence 15200, and count: 15132\n",
      "Currently at sequence 15300, and count: 15232\n",
      "Currently at sequence 15400, and count: 15332\n",
      "Model Tranlsated: [start] c g c e a e e a d d f b d b d c f a h h i j a d g k [end]\n",
      "Ground-Truth:     [start] c g c e a e e a d d f b d b d c f a h h i j a d g k [end]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently at sequence 15500, and count: 15432\n",
      "Currently at sequence 15600, and count: 15532\n",
      "Currently at sequence 15700, and count: 15632\n",
      "Currently at sequence 15800, and count: 15732\n",
      "Model Tranlsated: [start] b d b d c d a d e f c g b d c f a f i j b d a f k l a f h m a e ed a h d g ee [end]\n",
      "Ground-Truth:     [start] b d b d c d a d e f c g b d c f a f i j b d a f k l a f h m a e ed a h d g ee [end]\n",
      "Currently at sequence 15900, and count: 15832\n",
      "Currently at sequence 16000, and count: 15931\n",
      "Model Tranlsated: [start] c d b d a g d e c f a e g c d b d a g i j a h f h k [end]\n",
      "Ground-Truth:     [start] c d b d a g d e c f a e g c d b d a g i j a h f h k [end]\n",
      "Currently at sequence 16100, and count: 16031\n",
      "Currently at sequence 16200, and count: 16131\n",
      "Model Tranlsated: [start] c f c g a f d e c d b d b d a d h i a h f g j a e k b d b d a f m ed a d l ee [end]\n",
      "Ground-Truth:     [start] c f c g a f d e c d b d b d a d h i a h f g j a e k b d b d a f m ed a d l ee [end]\n",
      "Model Tranlsated: [start] c e c d b d a h d e f b d a d g h c f a g i j [end]\n",
      "Ground-Truth:     [start] c e c d b d a h d e f b d a d g h c f a g i j [end]\n",
      "Currently at sequence 16300, and count: 16231\n",
      "Model Tranlsated: [start] b d b d c g c e a f f g a d e h c f a h d i j c e a g k l a e m [end]\n",
      "Ground-Truth:     [start] b d b d c g c e a f f g a d e h c f a h d i j c e a g k l a e m [end]\n",
      "Currently at sequence 16400, and count: 16331\n",
      "Currently at sequence 16500, and count: 16430\n",
      "Model Tranlsated: [start] b d a e d b d a d e f b d a g g h b d b d c e a h j k l a g i m b d a d ed ee [end]\n",
      "Ground-Truth:     [start] b d a e d b d a d e f b d a g g h b d b d c e a h j k l a g i m b d a d ed ee [end]\n",
      "Model Tranlsated: [start] b d b d b d a g e f b d b d b d a h h i j a h d g k c f a d l m [end]\n",
      "Ground-Truth:     [start] b d b d b d a g e f b d b d b d a h h i j a h d g k c f a d l m [end]\n",
      "Model Tranlsated: [start] c d c d a d d e c d c d a h f g h b d b d a e k c d a e m b d a h l ed ee b d a f ef eg a e eh a f eh [end]\n",
      "Ground-Truth:     [start] c d c d a d d e c d c d a h f g h b d b d a e k c d a e m a h j l ed b d a f ee ef a e eg a f i eh [end]\n",
      "Currently at sequence 16600, and count: 16529\n",
      "Currently at sequence 16700, and count: 16628\n"
     ]
    }
   ],
   "source": [
    "token_gen = Generate_tokens()\n",
    "translated_texts = []\n",
    "print(f\"Total Number of Input Text Sequences: {len(test_input_txt)}\")\n",
    "correct_count = 0\n",
    "rand_idx = np.random.randint(0, high=len(test_input_txt)-1, size=100)\n",
    "for i in range(len(test_input_txt)):\n",
    "    if i % 100 == 0: print(f\"Currently at sequence {i}, and count: {correct_count}\")\n",
    "    translated_seq = token_gen(test_input_txt[i], target_vectorization,\n",
    "                     source_vectorization, model=t, max_out_seq_len=output_seq_len)\n",
    "    if i in rand_idx:\n",
    "        print(f\"Model Tranlsated: {translated_seq}\")\n",
    "        print(f\"Ground-Truth:     {test_target_txt[i]}\")\n",
    "    if translated_seq == test_target_txt[i]:\n",
    "        correct_count += 1\n",
    "    translated_texts.append(translated_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "618844f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.9957142857142857\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model Accuracy: {correct_count / len(translated_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24adb6fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
